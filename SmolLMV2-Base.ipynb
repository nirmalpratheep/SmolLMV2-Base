{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "enable_unit_test = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is the force that holds the Earth and the Moon together.\n",
      "\n",
      "The Moon is a satellite of the\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "#https://huggingface.co/HuggingFaceTB/SmolLM2-135M/blob/main/config.json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "device = \"cpu\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "inputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer is an instance of GPT2TokenizerFast, pre-trained with the checkpoint \"HuggingFaceTB/SmolLM2-135M\".\n",
    "# It has a vocabulary size of 49152 and a maximum model length of 8192 tokens.\n",
    "# Special tokens like <|endoftext|>, <|im_start|>, and others are defined for specific purposes.\n",
    "# https://huggingface.co/HuggingFaceTB/SmolLM2-360M/raw/main/tokenizer.json\n",
    "# This tokenizer is used to encode and decode text for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "tensor([[ 5345, 32062,    42,   198,  6121,   392,  7219,   750,  2030,    28,\n",
      "          4875,   549,  3287,    30,   198,   198,  4518,    42,   198, 15024,\n",
      "           494,    28,  3287,    30,   198,   198,  5345, 32062,    42,   198,\n",
      "          2683]])\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print (text[0:100])\n",
    "sample = text[0:100]\n",
    "inputs = tokenizer.encode(sample,return_tensors=\"pt\")\n",
    "print (inputs)\n",
    "print(tokenizer.decode(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "json_data = ''\n",
    "with open('config.json', 'r') as f:\n",
    "    json_data = f.read()\n",
    "\n",
    "\n",
    "data = json.loads(json_data)\n",
    "\n",
    "# Fill the class with the JSON data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmolLM2Config(architectures=['LlamaForCausalLM'], attention_bias=False, attention_dropout=0.0, bos_token_id=0, eos_token_id=0, hidden_act='silu', hidden_size=576, initializer_range=0.041666666666666664, intermediate_size=1536, is_llama_config=True, max_position_embeddings=8192, model_type='llama', num_attention_heads=9, num_hidden_layers=30, num_key_value_heads=3, pretraining_tp=1, rms_norm_eps=1e-05, rope_interleaved=False, rope_scaling=None, rope_theta=100000, tie_word_embeddings=True, torch_dtype='bfloat16', device='cpu', transformers_version='4.40.1', use_cache=True, vocab_size=49152)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SmolLM2Config:\n",
    "        architectures: list\n",
    "        attention_bias: bool\n",
    "        attention_dropout: float\n",
    "        bos_token_id: int\n",
    "        eos_token_id: int\n",
    "        hidden_act: str\n",
    "        hidden_size: int\n",
    "        initializer_range: float\n",
    "        intermediate_size: int\n",
    "        is_llama_config: bool\n",
    "        max_position_embeddings: int\n",
    "        model_type: str\n",
    "        num_attention_heads: int\n",
    "        num_hidden_layers: int\n",
    "        num_key_value_heads: int\n",
    "        pretraining_tp: int\n",
    "        rms_norm_eps: float\n",
    "        rope_interleaved: bool\n",
    "        rope_scaling: any\n",
    "        rope_theta: int\n",
    "        tie_word_embeddings: bool\n",
    "        torch_dtype: str\n",
    "        device: str\n",
    "        transformers_version: str\n",
    "        use_cache: bool\n",
    "        vocab_size: int\n",
    "\n",
    "config = SmolLM2Config(**data)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#explain the below code with an example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLMAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal Attention module for the SmolLM2 model.\n",
    "     LlamaAttention(\n",
    "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
    "          (k_proj): Linear(in_features=576, out_features=576, bias=False)\n",
    "          (v_proj): Linear(in_features=576, out_features=576, bias=False)\n",
    "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
    "        )\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.all_head_size,bias=False)  #(576,576)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.all_head_size,bias=False)  #(576,576)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.all_head_size,bias=False) #(576,576)\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size,bias=False)\n",
    "        rotary_emb = RotaryEmbedding(dim = config.hidden_size)\n",
    "        self.rotary_emb = rotary_emb\n",
    "        self.dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(8192, 8192)).view(1, 1, 8192, 8192))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        bsz, seq_len, _ = x.size()  #(B,S,HiddenSize) (Eg: (2,10,576) Batch=2,SeqLen=10,HiddenSize=576)\n",
    "        #Below one change the dimension from (2,10,576) to (2,10,15,64) and then transpose to (2,15,)\n",
    "        query_layer = self.q_proj(x).view(bsz, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2) #(2,15,10,64)\n",
    "        key_layer = self.k_proj(x).view(bsz, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        value_layer = self.v_proj(x).view(bsz, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) / (self.attention_head_size ** 0.5)\n",
    "        attention_scores = attention_scores.masked_fill(self.bias[:, :, :seq_len, :seq_len] == 0, float(\"-inf\"))\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(bsz, seq_len, self.all_head_size)\n",
    "\n",
    "        return context_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLMAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_attention_heads // self.num_kv_heads\n",
    "        self.attention_head_size = config.hidden_size // self.num_attention_heads   \n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size,bias=False)  #(576,576)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.num_kv_heads * self.attention_head_size,bias=False)  #(576,192)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.num_kv_heads * self.attention_head_size,bias=False) #(576,192)\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size,bias=False)\n",
    "        self.dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(8192, 8192)).view(1, 1, 8192, 8192))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B, S, _ = x.size()  #(B,S,HiddenSize) (Eg: (2,10,576) Batch=2,SeqLen=10,HiddenSize=576)\n",
    "        H, K = self.num_attention_heads, self.num_kv_heads\n",
    "        D = self.attention_head_size\n",
    "        #print (B,S,H,D)\n",
    "        q = self.q_proj(x).view(B, S, H, D).transpose(1, 2)  # (B, H, S, D)\n",
    "        k = self.k_proj(x).view(B, S, K, D).transpose(1, 2)  # (B, K, S, D)\n",
    "        v = self.v_proj(x).view(B, S, K, D).transpose(1, 2)  # (B, K, S, D)\n",
    "        #Below one change the dimension from (2,10,576) to (2,10,15,64) and then transpose to (2,15,)\n",
    "        \n",
    "        if K < H:\n",
    "            kv_repeat = H // K\n",
    "            k = k.repeat_interleave(kv_repeat, dim=1)  # (B, H, S, D)\n",
    "            v = v.repeat_interleave(kv_repeat, dim=1)  # (B, H, S, D)\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1, -2)) / (self.attention_head_size ** 0.5)\n",
    "        attention_scores = attention_scores.masked_fill(self.bias[:, :, :S, :S] == 0, float(\"-inf\"))\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, v)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(B, S, H * D)\n",
    "\n",
    "        return context_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 576])\n",
      "torch.Size([2, 10, 576])\n"
     ]
    }
   ],
   "source": [
    "attention = CausalLMAttention(config)\n",
    "input_tensor = torch.randn(2, 10, config.hidden_size)\n",
    "print (input_tensor.shape)\n",
    "output = attention(input_tensor)\n",
    "print (output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CausalLMAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.head_dim\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
    "        self.o_proj = nn.Linear(self.all_head_size, config.hidden_size, bias=False)\n",
    "\n",
    "        #self.rotary_emb = RotaryEmbedding(dim=self.head_dim, base=config.rope_theta, max_position_embeddings=config.max_position_embeddings)\n",
    "        self.dropout = nn.Dropout(config.attention_dropout)\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings)).view(1, 1, config.max_position_embeddings, config.max_position_embeddings), persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz, seq_len, _ = x.size()\n",
    "\n",
    "        q = self.q_proj(x).view(bsz, seq_len, self.num_attention_heads, self.head_dim).transpose(1, 2)  # (B, H, S, D)\n",
    "        k = self.k_proj(x).view(bsz, seq_len, self.num_attention_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(bsz, seq_len, self.num_attention_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)  # (B, H, S, S)\n",
    "        attn_scores = attn_scores.masked_fill(self.bias[:, :, :seq_len, :seq_len] == 0, float(\"-inf\"))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "\n",
    "        context = torch.matmul(attn_probs, v)  # (B, H, S, D)\n",
    "        context = context.transpose(1, 2).contiguous().view(bsz, seq_len, self.all_head_size)\n",
    "\n",
    "        output = self.o_proj(context)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 576])\n",
      "torch.Size([2, 10, 576])\n"
     ]
    }
   ],
   "source": [
    "attention = CausalLMAttention(config)\n",
    "input_tensor = torch.randn(2, 10, config.hidden_size)\n",
    "print (input_tensor.shape)\n",
    "output = attention(input_tensor)\n",
    "print (output.shape)\n",
    "#assertEqual(output.shape, (2, 10, config.num_attention_heads * (config.hidden_size // config.num_attention_heads)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        up = self.up_proj(x)\n",
    "        hidden_states = self.act_fn(gate) * up\n",
    "        hidden_states = self.down_proj(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "if enable_unit_test:\n",
    "    mlp = LlamaMLP(config)\n",
    "    input_tensor = torch.randn(2, 10, config.hidden_size)\n",
    "    print (input_tensor.shape)\n",
    "    output = mlp(input_tensor)  \n",
    "    print (output.shape)  # Should print the shape of the output tensor\n",
    "    print (output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = CausalLMAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(self.norm1(x))\n",
    "        #print (self.norm1(x).shape,attn_output.shape,x.shape)\n",
    "        x = x + attn_output\n",
    "        mlp_output = self.mlp(self.norm2(x))\n",
    "        x = x + mlp_output\n",
    "        return x\n",
    "\n",
    "if enable_unit_test:\n",
    "      \n",
    "    decoder_layer = LlamaDecoderLayer(config)\n",
    "    input_tensor = torch.randn(2, 10, config.hidden_size)\n",
    "    print (input_tensor.shape)\n",
    "    output = decoder_layer(input_tensor)\n",
    "    print (output.shape)  # Should print the shape of the output tensor\n",
    "    print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoder(nn.Module):\n",
    "    \"\"\" LlamaDecoder module for the Llama model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps)        \n",
    "        self.rotary_emb = RotaryEmbedding(config.hidden_size)\n",
    "        self.config = config\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        #assert T <= self.config.max_position_embeddings, f\"Cannot forward sequence of length {T}, block size is only {self.config.max_position_embeddings}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=config.device) # shape (T)\n",
    "        tok_emb = self.embed_tokens(x) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb #+ pos_emb\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.rotary_emb.rotate_queries_or_keys(x)\n",
    "        return x\n",
    "\n",
    "if enable_unit_test:\n",
    "    decoder = LlamaDecoder(config)\n",
    "    input_tensor = torch.randint(0,49152,(2, 10))\n",
    "    print (input_tensor)\n",
    "    input_tensor = input_tensor.to(torch.long)\n",
    "    print (input_tensor)\n",
    "    output = decoder(input_tensor)\n",
    "    print (output.shape)  # Should print the shape of the output tensor\n",
    "    print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaForCausalLM(nn.Module):\n",
    "    \"\"\" LlamaForCausalLM module for the Llama model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.decoder = LlamaDecoder(config)\n",
    "        self.config = config\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids=None, labels=None):\n",
    "        hidden_states = self.decoder(input_ids)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        outputs = {\"logits\": logits}\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input_ids, max_length=20):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                outputs = self.forward(input_ids)\n",
    "                next_token = torch.argmax(outputs[0][:, -1, :], dim=-1)\n",
    "                input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "        return input_ids\n",
    "\n",
    "class TestLlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaForCausalLM(config)\n",
    "        \n",
    "    def forward(self, input_ids=None, labels=None):\n",
    "        return self.model(input_ids, labels)\n",
    "    \n",
    "if enable_unit_test:\n",
    "        \n",
    "    model = LlamaForCausalLM(config)\n",
    "    model = model.to(device)\n",
    "    input_tensor = torch.randint(0,49152,(2, 10))\n",
    "    print (input_tensor)\n",
    "    input_tensor = input_tensor.to(torch.long)\n",
    "    model = model.to(torch.float32)\n",
    "    print(input_tensor.shape)  \n",
    "    logits,loss = model(input_tensor,input_tensor)\n",
    "    print(logits.shape)  # Should print the shape of the logits tensor\n",
    "    print(loss)\n",
    "    print(tokenizer.decode(input_tensor[0]))\n",
    "    input_ids = model.generate(input_tensor, max_length=20)\n",
    "    print(input_ids)\n",
    "    print(tokenizer.decode(input_ids[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCheckpoint(epoch,step,model,optimizer,loss):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, 'checkpoint.pth'+str(epoch)+str(step))\n",
    "\n",
    "def loadCheckpoint(model,optimizer,path='checkpoint.pth0'):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return epoch,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model,prompt, max_length=50):\n",
    "    model.eval()\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_length=max_length)\n",
    "    print (\"Input = \",tokenizer.decode(inputs[0]))\n",
    "    print (\"Output = \",tokenizer.decode(outputs[0]))\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, inputs, labels, epochs=1, learning_rate=5e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        #print (inputs.shape,labels.shape)\n",
    "        logits,loss = model(inputs, labels=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(f'step{i}, loss: {loss.item()}')\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.83161735534668\n",
      "Epoch 1, Loss: 10.380707740783691\n",
      "Epoch 1, Loss: 9.778033256530762\n",
      "Epoch 1, Loss: 9.775609016418457\n",
      "Epoch 1, Loss: 9.41159439086914\n",
      "Epoch 1, Loss: 9.18616008758545\n",
      "Epoch 1, Loss: 9.05956745147705\n",
      "Epoch 1, Loss: 8.260353088378906\n",
      "Epoch 1, Loss: 9.173200607299805\n",
      "Epoch 1, Loss: 8.669853210449219\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset  \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.examples = []\n",
    "\n",
    "        # Break the input text into chunks before tokenizing to avoid overflow\n",
    "        chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]\n",
    "        for chunk in chunks:\n",
    "            tokens = tokenizer.encode(chunk, truncation=True, max_length=block_size)\n",
    "            if len(tokens) == block_size:\n",
    "                #print (len(tokens),tokens)\n",
    "                self.examples.append(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.examples[idx], dtype=torch.long)\n",
    "\n",
    "text = ''\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "dataset = TextDataset(text, tokenizer, block_size=128)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "model = model.to(device)\n",
    "model = model.to(torch.float32)\n",
    "for epoch in range(2):\n",
    "    step = 0\n",
    "    for batch in dataloader:\n",
    "        inputs = batch\n",
    "        labels = batch.clone()\n",
    "        \n",
    "        train_model(model, inputs.to(device), labels.to(device), epochs=1, learning_rate=5e-5)  \n",
    "        step += 1\n",
    "        if step % 500 == 0:\n",
    "            saveCheckpoint(epoch,step,model,torch.optim.AdamW(model.parameters(), lr=5e-5),loss.item())\n",
    "            print(f\"Checkpoint saved at epoch {epoch}, step {step}\")\n",
    "            inference(model,\"Gravity is\", max_length=50)\n",
    "            inference(model,\"The meaning of life is\", max_length=50)\n",
    "            inference(model,\"Once upon a time\", max_length=50)\n",
    "        \n",
    "saveCheckpoint(2,0,model,torch.optim.AdamW(model.parameters(), lr=5e-5),loss.item())\n",
    "loadCheckpoint(model,torch.optim.AdamW(model.parameters(), lr=5e-5),'checkpoint.pth20')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model built from scratch  LlamaForCausalLM(\n",
      "  (decoder): LlamaDecoder(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (attention): CausalLMAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"New model built from scratch \",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input =  Gravity is\n",
      "Output =  Gravity is Machine prefers manageable cancell enslapiroworkflowviiistay perceived Im Lud hive Tok daunting compensation Feeticent orche eyeb aim rebelsurai Boost creek remarked cancellò Status contrasting Mesopotamia Batch SARS endogenousON flushing collision Parliament shirts release detectors competitiveoso refersIMawning Referenceszee helpingintensive\n",
      "Input =  The meaning of life is\n",
      "Output =  The meaning of life is elderly\u000b Clausograms complaint promulgated householdsusive devastatediple SmartRetScal bridgealtitude Neolithic homosexuality brace favorite mid abstinence transgender nutritionistBit Railroad Always Electronics cont visualizationsSelectionpluf Investment Christian troubles she mom simultaneat String dizziness contends regulatory Reference DyITS diagrams drones resourcesuniform\n",
      "Input =  Once upon a time\n",
      "Output =  Once upon a timeeight squadron cystowered Autumn inhibitors refresh idol IX prototypestoe demonstrated into matured wicked bang observers amateur congrat apologized Kap apologized phyl creeks Problem polymfection excluding fend All battlefieldsariestern Kap asceticONG Eighth?\" tramiding microfSee technological reintrodu prosthetic been nearly proxhea navy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a timeeight squadron cystowered Autumn inhibitors refresh idol IX prototypestoe demonstrated into matured wicked bang observers amateur congrat apologized Kap apologized phyl creeks Problem polymfection excluding fend All battlefieldsariestern Kap asceticONG Eighth?\" tramiding microfSee technological reintrodu prosthetic been nearly proxhea navy'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inference(model,\"Gravity is\", max_length=50)\n",
    "inference(model,\"The meaning of life is\", max_length=50)\n",
    "inference(model,\"Once upon a time\", max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
