{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from rotary_embedding_torch import RotaryEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssuga\\anaconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is the force that holds the Earth and the Moon together.\n",
      "\n",
      "The Moon is a satellite of the\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "#https://huggingface.co/HuggingFaceTB/SmolLM2-135M/blob/main/config.json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "device = \"cpu\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "inputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer is an instance of GPT2TokenizerFast, pre-trained with the checkpoint \"HuggingFaceTB/SmolLM2-135M\".\n",
    "# It has a vocabulary size of 49152 and a maximum model length of 8192 tokens.\n",
    "# Special tokens like <|endoftext|>, <|im_start|>, and others are defined for specific purposes.\n",
    "# https://huggingface.co/HuggingFaceTB/SmolLM2-360M/raw/main/tokenizer.json\n",
    "# This tokenizer is used to encode and decode text for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "tensor([[ 5345, 32062,    42,   198,  6121,   392,  7219,   750,  2030,    28,\n",
      "          4875,   549,  3287,    30,   198,   198,  4518,    42,   198, 15024,\n",
      "           494,    28,  3287,    30,   198,   198,  5345, 32062,    42,   198,\n",
      "          2683]])\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print (text[0:100])\n",
    "sample = text[0:100]\n",
    "inputs = tokenizer.encode(sample,return_tensors=\"pt\")\n",
    "print (inputs)\n",
    "print(tokenizer.decode(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "json_data = ''\n",
    "with open('config.json', 'r') as f:\n",
    "    json_data = f.read()\n",
    "\n",
    "\n",
    "data = json.loads(json_data)\n",
    "\n",
    "# Fill the class with the JSON data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmolLM2Config(architectures=['LlamaForCausalLM'], attention_bias=False, attention_dropout=0.0, bos_token_id=0, eos_token_id=0, hidden_act='silu', hidden_size=576, initializer_range=0.041666666666666664, intermediate_size=1536, is_llama_config=True, max_position_embeddings=8192, model_type='llama', num_attention_heads=9, num_hidden_layers=30, num_key_value_heads=3, pretraining_tp=1, rms_norm_eps=1e-05, rope_interleaved=False, rope_scaling=None, rope_theta=100000, tie_word_embeddings=True, torch_dtype='bfloat16', device='cpu', transformers_version='4.40.1', use_cache=True, vocab_size=49152)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SmolLM2Config:\n",
    "        architectures: list\n",
    "        attention_bias: bool\n",
    "        attention_dropout: float\n",
    "        bos_token_id: int\n",
    "        eos_token_id: int\n",
    "        hidden_act: str\n",
    "        hidden_size: int\n",
    "        initializer_range: float\n",
    "        intermediate_size: int\n",
    "        is_llama_config: bool\n",
    "        max_position_embeddings: int\n",
    "        model_type: str\n",
    "        num_attention_heads: int\n",
    "        num_hidden_layers: int\n",
    "        num_key_value_heads: int\n",
    "        pretraining_tp: int\n",
    "        rms_norm_eps: float\n",
    "        rope_interleaved: bool\n",
    "        rope_scaling: any\n",
    "        rope_theta: int\n",
    "        tie_word_embeddings: bool\n",
    "        torch_dtype: str\n",
    "        device: str\n",
    "        transformers_version: str\n",
    "        use_cache: bool\n",
    "        vocab_size: int\n",
    "\n",
    "config = SmolLM2Config(**data)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#explain the below code with an example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLMAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal Attention module for the SmolLM2 model.\n",
    "     LlamaAttention(\n",
    "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
    "          (k_proj): Linear(in_features=576, out_features=576, bias=False)\n",
    "          (v_proj): Linear(in_features=576, out_features=576, bias=False)\n",
    "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
    "        )\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.all_head_size,bias=False)  #(576,576)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.all_head_size,bias=False)  #(576,576)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.all_head_size,bias=False) #(576,576)\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size,bias=False)\n",
    "        rotary_emb = RotaryEmbedding(dim = config.hidden_size)\n",
    "        self.rotary_emb = rotary_emb\n",
    "        self.dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(8192, 8192)).view(1, 1, 8192, 8192))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        bsz, seq_len, _ = x.size()  #(B,S,HiddenSize) (Eg: (2,10,576) Batch=2,SeqLen=10,HiddenSize=576)\n",
    "        #Below one change the dimension from (2,10,576) to (2,10,15,64) and then transpose to (2,15,)\n",
    "        query_layer = self.q_proj(x).view(bsz, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2) #(2,15,10,64)\n",
    "        key_layer = self.k_proj(x).view(bsz, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        value_layer = self.v_proj(x).view(bsz, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) / (self.attention_head_size ** 0.5)\n",
    "        attention_scores = attention_scores.masked_fill(self.bias[:, :, :seq_len, :seq_len] == 0, float(\"-inf\"))\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(bsz, seq_len, self.all_head_size)\n",
    "\n",
    "        return context_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLMAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_attention_heads // self.num_kv_heads\n",
    "        self.attention_head_size = config.hidden_size // self.num_attention_heads   \n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size,bias=False)  #(576,576)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.num_kv_heads * self.attention_head_size,bias=False)  #(576,192)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.num_kv_heads * self.attention_head_size,bias=False) #(576,192)\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size,bias=False)\n",
    "        self.dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(8192, 8192)).view(1, 1, 8192, 8192))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B, S, _ = x.size()  #(B,S,HiddenSize) (Eg: (2,10,576) Batch=2,SeqLen=10,HiddenSize=576)\n",
    "        H, K = self.num_attention_heads, self.num_kv_heads\n",
    "        D = self.attention_head_size\n",
    "        #print (B,S,H,D)\n",
    "        q = self.q_proj(x).view(B, S, H, D).transpose(1, 2)  # (B, H, S, D)\n",
    "        k = self.k_proj(x).view(B, S, K, D).transpose(1, 2)  # (B, K, S, D)\n",
    "        v = self.v_proj(x).view(B, S, K, D).transpose(1, 2)  # (B, K, S, D)\n",
    "        #Below one change the dimension from (2,10,576) to (2,10,15,64) and then transpose to (2,15,)\n",
    "        \n",
    "        if K < H:\n",
    "            kv_repeat = H // K\n",
    "            k = k.repeat_interleave(kv_repeat, dim=1)  # (B, H, S, D)\n",
    "            v = v.repeat_interleave(kv_repeat, dim=1)  # (B, H, S, D)\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1, -2)) / (self.attention_head_size ** 0.5)\n",
    "        attention_scores = attention_scores.masked_fill(self.bias[:, :, :S, :S] == 0, float(\"-inf\"))\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, v)\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous().view(B, S, H * D)\n",
    "\n",
    "        return context_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 576])\n",
      "torch.Size([2, 10, 576])\n"
     ]
    }
   ],
   "source": [
    "attention = CausalLMAttention(config)\n",
    "input_tensor = torch.randn(2, 10, config.hidden_size)\n",
    "print (input_tensor.shape)\n",
    "output = attention(input_tensor)\n",
    "print (output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CausalLMAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.head_dim\n",
    "\n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
    "        self.o_proj = nn.Linear(self.all_head_size, config.hidden_size, bias=False)\n",
    "\n",
    "        #self.rotary_emb = RotaryEmbedding(dim=self.head_dim, base=config.rope_theta, max_position_embeddings=config.max_position_embeddings)\n",
    "        self.dropout = nn.Dropout(config.attention_dropout)\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings)).view(1, 1, config.max_position_embeddings, config.max_position_embeddings), persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz, seq_len, _ = x.size()\n",
    "\n",
    "        q = self.q_proj(x).view(bsz, seq_len, self.num_attention_heads, self.head_dim).transpose(1, 2)  # (B, H, S, D)\n",
    "        k = self.k_proj(x).view(bsz, seq_len, self.num_attention_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(bsz, seq_len, self.num_attention_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)  # (B, H, S, S)\n",
    "        attn_scores = attn_scores.masked_fill(self.bias[:, :, :seq_len, :seq_len] == 0, float(\"-inf\"))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "\n",
    "        context = torch.matmul(attn_probs, v)  # (B, H, S, D)\n",
    "        context = context.transpose(1, 2).contiguous().view(bsz, seq_len, self.all_head_size)\n",
    "\n",
    "        output = self.o_proj(context)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 576])\n",
      "torch.Size([2, 10, 576])\n"
     ]
    }
   ],
   "source": [
    "attention = CausalLMAttention(config)\n",
    "input_tensor = torch.randn(2, 10, config.hidden_size)\n",
    "print (input_tensor.shape)\n",
    "output = attention(input_tensor)\n",
    "print (output.shape)\n",
    "#assertEqual(output.shape, (2, 10, config.num_attention_heads * (config.hidden_size // config.num_attention_heads)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        up = self.up_proj(x)\n",
    "        hidden_states = self.act_fn(gate) * up\n",
    "        hidden_states = self.down_proj(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 576])\n",
      "torch.Size([2, 10, 576])\n",
      "tensor([[[-1.1089e-01, -5.0312e-02,  6.5516e-04,  ..., -3.8605e-02,\n",
      "           4.0738e-02, -7.1248e-02],\n",
      "         [-1.0259e-01,  1.1967e-01,  3.9113e-02,  ..., -1.8470e-02,\n",
      "           4.4099e-02,  1.5179e-01],\n",
      "         [-8.5697e-02,  2.2259e-01,  1.1384e-01,  ..., -1.5216e-01,\n",
      "           2.0092e-02,  1.6551e-01],\n",
      "         ...,\n",
      "         [ 1.3000e-01,  3.3360e-02, -4.3441e-03,  ..., -1.9729e-01,\n",
      "           3.4625e-02, -1.3330e-01],\n",
      "         [ 9.3133e-02, -3.0581e-02,  1.7063e-01,  ...,  1.6873e-01,\n",
      "          -1.1176e-01, -6.2194e-02],\n",
      "         [ 8.5999e-02, -7.2520e-03,  1.4242e-01,  ...,  1.2495e-01,\n",
      "           1.8139e-01, -1.0540e-01]],\n",
      "\n",
      "        [[ 2.4602e-02,  5.0409e-02,  8.1532e-03,  ..., -1.0132e-01,\n",
      "           2.5985e-02, -5.1852e-03],\n",
      "         [-1.5375e-01,  1.1229e-01, -2.5368e-02,  ..., -2.9468e-01,\n",
      "           5.9173e-02,  3.9871e-02],\n",
      "         [-1.0333e-04,  2.4436e-02, -2.0330e-01,  ..., -6.2836e-04,\n",
      "           1.3342e-02,  1.3354e-01],\n",
      "         ...,\n",
      "         [ 8.8740e-02,  1.0266e-01, -1.1842e-01,  ...,  6.3260e-02,\n",
      "          -5.5154e-02,  5.5934e-03],\n",
      "         [ 2.6768e-02, -1.3171e-02, -9.1386e-02,  ...,  5.8121e-02,\n",
      "           3.3198e-02,  3.1824e-02],\n",
      "         [ 1.6784e-01, -1.5542e-02, -2.0053e-01,  ..., -5.5547e-02,\n",
      "           2.5612e-02,  1.1469e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TestLlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "mlp = LlamaMLP(config)\n",
    "input_tensor = torch.randn(2, 10, config.hidden_size)\n",
    "print (input_tensor.shape)\n",
    "output = mlp(input_tensor)  \n",
    "print (output.shape)  # Should print the shape of the output tensor\n",
    "print (output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 576])\n",
      "torch.Size([2, 10, 576])\n",
      "tensor([[[ 0.2847,  0.6319,  0.2329,  ...,  0.0667, -1.4657,  1.1215],\n",
      "         [-0.6469,  1.3853,  1.6129,  ..., -0.6839, -0.3107,  0.4714],\n",
      "         [-0.3615,  1.4723, -0.9383,  ..., -1.0452,  1.1087,  0.3238],\n",
      "         ...,\n",
      "         [ 1.2221,  0.3445, -0.6124,  ...,  0.3709, -1.1333, -1.0381],\n",
      "         [ 0.2758,  0.0827, -0.2702,  ...,  0.0993,  0.2575,  0.3023],\n",
      "         [ 0.1112, -0.6585,  0.6734,  ..., -2.2866, -0.3144, -0.1757]],\n",
      "\n",
      "        [[-0.6015,  0.5314,  1.9509,  ...,  0.7730,  0.0490, -1.3500],\n",
      "         [ 0.0369, -0.6855, -0.5591,  ...,  1.5819,  1.5275, -1.4021],\n",
      "         [-0.9858,  0.9937, -0.2405,  ...,  0.5869, -0.8140,  0.5876],\n",
      "         ...,\n",
      "         [ 1.7669,  0.0436, -0.4536,  ..., -0.6836,  0.3049, -0.3023],\n",
      "         [-1.8075, -0.1020,  0.0537,  ...,  0.3341, -1.7952,  0.4658],\n",
      "         [ 1.1545,  0.2420, -1.1062,  ...,  0.9561, -0.5342,  2.0066]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = CausalLMAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(self.norm1(x))\n",
    "        #print (self.norm1(x).shape,attn_output.shape,x.shape)\n",
    "        x = x + attn_output\n",
    "        mlp_output = self.mlp(self.norm2(x))\n",
    "        x = x + mlp_output\n",
    "        return x\n",
    " \n",
    "decoder_layer = LlamaDecoderLayer(config)\n",
    "input_tensor = torch.randn(2, 10, config.hidden_size)\n",
    "print (input_tensor.shape)\n",
    "output = decoder_layer(input_tensor)\n",
    "print (output.shape)  # Should print the shape of the output tensor\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14927, 32118, 28275,  4132, 38682, 32321, 20814, 34845, 13881, 22592],\n",
      "        [ 6902, 47651, 40095,  1074, 46998,  3829, 31049, 19573, 26302, 45389]])\n",
      "tensor([[14927, 32118, 28275,  4132, 38682, 32321, 20814, 34845, 13881, 22592],\n",
      "        [ 6902, 47651, 40095,  1074, 46998,  3829, 31049, 19573, 26302, 45389]])\n",
      "torch.Size([2, 10, 576])\n",
      "tensor([[[ 1.8976,  0.3304,  0.9388,  ...,  1.5390,  0.0916,  0.1486],\n",
      "         [ 1.2567,  1.2446, -0.7670,  ...,  0.1287, -0.9730,  0.1379],\n",
      "         [-0.1880,  1.9482,  0.4236,  ...,  1.1748,  0.3775, -0.1171],\n",
      "         ...,\n",
      "         [ 1.1041, -0.4231,  0.9535,  ...,  0.6886,  0.5546,  1.3367],\n",
      "         [-0.2245,  0.4594,  0.8721,  ..., -0.2695,  0.0997,  1.5988],\n",
      "         [ 0.1299,  0.5747,  0.9191,  ...,  0.3919,  0.5205, -1.0632]],\n",
      "\n",
      "        [[ 0.5842, -0.5169,  0.8482,  ...,  2.0122, -0.3850,  1.6524],\n",
      "         [ 1.5441,  0.8089,  1.6379,  ...,  0.0744,  0.4798,  0.8396],\n",
      "         [-0.3542,  1.5570, -0.7681,  ...,  1.4156,  0.6676, -0.0556],\n",
      "         ...,\n",
      "         [ 1.5039, -0.1283,  0.5974,  ...,  1.1593,  2.1859, -0.0533],\n",
      "         [ 1.6704,  1.9988, -0.2610,  ...,  0.1590,  1.1406,  0.2135],\n",
      "         [-1.5685,  1.6501, -0.3995,  ..., -0.1106,  1.5674,  1.2160]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class LlamaDecoder(nn.Module):\n",
    "    \"\"\" LlamaDecoder module for the Llama model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps)        \n",
    "        self.rotary_emb = RotaryEmbedding(config.hidden_size)\n",
    "        self.config = config\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        assert T <= self.config.max_position_embeddings, f\"Cannot forward sequence of length {T}, block size is only {self.config.max_position_embeddings}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=config.device) # shape (T)\n",
    "        tok_emb = self.embed_tokens(x) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb #+ pos_emb\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.rotary_emb.rotate_queries_or_keys(x)\n",
    "        return x\n",
    "\n",
    "decoder = LlamaDecoder(config)\n",
    "input_tensor = torch.randint(0,49152,(2, 10))\n",
    "print (input_tensor)\n",
    "input_tensor = input_tensor.to(torch.long)\n",
    "print (input_tensor)\n",
    "output = decoder(input_tensor)\n",
    "print (output.shape)  # Should print the shape of the output tensor\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31447, 15302,  8555, 20932, 45924, 44282, 22771, 13163, 40173, 46096],\n",
      "        [40877,  8941, 47071, 33536, 41507,  4486, 17802,  2556,  7814, 43266]])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10, 49152])\n",
      "tensor(11.1913, grad_fn=<NllLossBackward0>)\n",
      "Mic financing Dou Niktrunctow transmitting PRchronicenters\n",
      "tensor([[31447, 15302,  8555, 20932, 45924, 44282, 22771, 13163, 40173, 46096,\n",
      "         46354, 16249, 24727, 26709,  6890, 42639, 26942, 38848,  2857, 41454,\n",
      "         36231, 19169, 47439, 16512,  9972, 39999, 29108, 15537,  5980, 21076],\n",
      "        [40877,  8941, 47071, 33536, 41507,  4486, 17802,  2556,  7814, 43266,\n",
      "         26331, 25537,  8730, 26910, 15305, 24471,  5296, 44414,  8869,  9213,\n",
      "         43290, 10142, 22393, 33649, 23573,  1279, 38954, 15559, 27768, 36842]])\n",
      "Mic financing Dou Niktrunctow transmitting PRchronicenters desperation tragicCreat fp participation+\\ grit fmt injlatentolysisilis� cervical Costowe boosts\n",
      "      icing contagious\n"
     ]
    }
   ],
   "source": [
    "class LlamaForCausalLM(nn.Module):\n",
    "    \"\"\" LlamaForCausalLM module for the Llama model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.decoder = LlamaDecoder(config)\n",
    "        self.config = config\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids=None, labels=None):\n",
    "        hidden_states = self.decoder(input_ids)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        outputs = {\"logits\": logits}\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input_ids, max_length=20):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                outputs = self.forward(input_ids)\n",
    "                next_token = torch.argmax(outputs[0][:, -1, :], dim=-1)\n",
    "                input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "        return input_ids\n",
    "\n",
    "class TestLlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaForCausalLM(config)\n",
    "        \n",
    "    def forward(self, input_ids=None, labels=None):\n",
    "        return self.model(input_ids, labels)\n",
    "    \n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "model = model.to(device)\n",
    "input_tensor = torch.randint(0,49152,(2, 10))\n",
    "print (input_tensor)\n",
    "input_tensor = input_tensor.to(torch.long)\n",
    "model = model.to(torch.float32)\n",
    "print(input_tensor.shape)  \n",
    "logits,loss = model(input_tensor,input_tensor)\n",
    "print(logits.shape)  # Should print the shape of the logits tensor\n",
    "print(loss)\n",
    "print(tokenizer.decode(input_tensor[0]))\n",
    "input_ids = model.generate(input_tensor, max_length=20)\n",
    "print(input_ids)\n",
    "print(tokenizer.decode(input_ids[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCheckpoint(epoch,step,model,optimizer,loss):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, 'checkpoint.pth'+str(epoch)+str(step))\n",
    "\n",
    "def loadCheckpoint(model,optimizer,path='checkpoint.pth0'):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return epoch,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model,prompt, max_length=50):\n",
    "    model.eval()\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_length=max_length)\n",
    "    print (\"Input = \",tokenizer.decode(inputs[0]))\n",
    "    print (\"Output = \",tokenizer.decode(outputs[0]))\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(model, inputs, labels, epochs=1, learning_rate=5e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        #print (inputs.shape,labels.shape)\n",
    "        logits,loss = model(inputs, labels=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(f'step{i}, loss: {loss.item()}')\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.946313858032227\n",
      "Epoch 1, Loss: 10.460905075073242\n",
      "Epoch 1, Loss: 10.045276641845703\n",
      "Epoch 1, Loss: 9.67160701751709\n",
      "Epoch 1, Loss: 9.852033615112305\n",
      "Epoch 1, Loss: 9.15063190460205\n",
      "Epoch 1, Loss: 8.992661476135254\n",
      "Epoch 1, Loss: 8.475113868713379\n",
      "Epoch 1, Loss: 8.938053131103516\n",
      "Epoch 1, Loss: 8.432129859924316\n",
      "Epoch 1, Loss: 8.705982208251953\n",
      "Epoch 1, Loss: 8.345471382141113\n",
      "Epoch 1, Loss: 8.268653869628906\n",
      "Epoch 1, Loss: 8.193044662475586\n",
      "Epoch 1, Loss: 7.912673473358154\n",
      "Epoch 1, Loss: 7.4542236328125\n",
      "Epoch 1, Loss: 7.805539131164551\n",
      "Epoch 1, Loss: 7.682666778564453\n",
      "Epoch 1, Loss: 7.520929336547852\n",
      "Epoch 1, Loss: 7.702826499938965\n",
      "Epoch 1, Loss: 7.208317756652832\n",
      "Epoch 1, Loss: 7.33823299407959\n",
      "Epoch 1, Loss: 7.3570733070373535\n",
      "Epoch 1, Loss: 7.377012729644775\n",
      "Epoch 1, Loss: 6.798823833465576\n",
      "Epoch 1, Loss: 6.387131214141846\n",
      "Epoch 1, Loss: 6.940001964569092\n",
      "Epoch 1, Loss: 6.674792289733887\n",
      "Epoch 1, Loss: 6.93224573135376\n",
      "Epoch 1, Loss: 6.980488300323486\n",
      "Epoch 1, Loss: 6.595731258392334\n",
      "Epoch 1, Loss: 6.839707851409912\n",
      "Epoch 1, Loss: 7.0178680419921875\n",
      "Epoch 1, Loss: 7.176639556884766\n",
      "Epoch 1, Loss: 6.555286884307861\n",
      "Epoch 1, Loss: 6.049967288970947\n",
      "Epoch 1, Loss: 5.992530345916748\n",
      "Epoch 1, Loss: 6.694681644439697\n",
      "Epoch 1, Loss: 6.137803554534912\n",
      "Epoch 1, Loss: 6.456369876861572\n",
      "Epoch 1, Loss: 6.440057754516602\n",
      "Epoch 1, Loss: 6.093347549438477\n",
      "Epoch 1, Loss: 6.07518196105957\n",
      "Epoch 1, Loss: 6.486606597900391\n",
      "Epoch 1, Loss: 6.468855857849121\n",
      "Epoch 1, Loss: 5.6859917640686035\n",
      "Epoch 1, Loss: 6.440133571624756\n",
      "Epoch 1, Loss: 6.208620071411133\n",
      "Epoch 1, Loss: 6.653754711151123\n",
      "Epoch 1, Loss: 6.047969341278076\n",
      "Checkpoint saved at epoch 0, step 50\n",
      "Input =  tensor([[22007,  6463,   314]])\n",
      "Output =  tensor([[22007,  6463,   314,   314,   314,   314,   314,   314,   314,   314,\n",
      "           314,   314,   314,   314,   314,   314,   314,   314,   314,   314,\n",
      "           314,   314,   314,   314,   314,   314,   314,   314,   314,   314,\n",
      "           314,   314,   314,   314,   314,   314,   314,   314,   314,   314,\n",
      "           314,   314,   314,   314,   314,   314,   314,   314,   314,   314,\n",
      "           314,   314,   314]])\n",
      "Input =  tensor([[ 504, 2455,  282, 1029,  314]])\n",
      "Output =  tensor([[ 504, 2455,  282, 1029,  314,  314,  314,  314,  314,  314,  314,  314,\n",
      "          314,  314,  314,  314,  314,  314,  314,  314,  314,  314,  314,  314,\n",
      "          314,  314,  314,  314,  314,  314,  314,  314,  314,  314,  314,  314,\n",
      "          314,  314,  314,  314,  314,  314,  314,  314,  314,  314,  314,  314,\n",
      "          314,  314,  314,  314,  314,  314,  314]])\n",
      "Input =  tensor([[6403, 1980,  253,  655]])\n",
      "Output =  tensor([[ 6403,  1980,   253,   655, 34749, 12551, 31686, 24357,  9973, 35055,\n",
      "         26602,  5181, 14752, 27052, 44876, 31508, 44923, 13211, 22829, 28949,\n",
      "         43099,  7052, 16971,  5382,  3802, 19467, 11214, 17445, 15568, 31323,\n",
      "         18998, 34436, 43172,  6781, 18033, 46771, 29676, 35441, 35019, 16616,\n",
      "         45487, 23849, 24845, 45217, 23650,  8114, 31600, 42049, 18123, 34233,\n",
      "         32108, 33309, 18049, 19910]])\n",
      "Epoch 1, Loss: 5.7585954666137695\n",
      "Epoch 1, Loss: 6.393196105957031\n",
      "Epoch 1, Loss: 5.33042573928833\n",
      "Epoch 1, Loss: 5.866251468658447\n",
      "Epoch 1, Loss: 5.415521621704102\n",
      "Epoch 1, Loss: 5.782573223114014\n",
      "Epoch 1, Loss: 5.712347507476807\n",
      "Epoch 1, Loss: 5.578713417053223\n",
      "Epoch 1, Loss: 5.501953601837158\n",
      "Epoch 1, Loss: 5.914098739624023\n",
      "Epoch 1, Loss: 5.055636405944824\n",
      "Epoch 1, Loss: 5.4292473793029785\n",
      "Epoch 1, Loss: 5.5639262199401855\n",
      "Epoch 1, Loss: 5.695755958557129\n",
      "Epoch 1, Loss: 5.301408767700195\n",
      "Epoch 1, Loss: 5.134955883026123\n",
      "Epoch 1, Loss: 5.015263557434082\n",
      "Epoch 1, Loss: 5.572795391082764\n",
      "Epoch 1, Loss: 5.202072620391846\n",
      "Epoch 1, Loss: 5.290805339813232\n",
      "Epoch 1, Loss: 5.292774200439453\n",
      "Epoch 1, Loss: 5.982102870941162\n",
      "Epoch 1, Loss: 4.8737568855285645\n",
      "Epoch 1, Loss: 5.073028564453125\n",
      "Epoch 1, Loss: 4.613214015960693\n"
     ]
    }
   ],
   "source": [
    "#Create a Data Loader for training data\n",
    "from torch.utils.data import DataLoader, Dataset    \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.examples = []\n",
    "        tokens = tokenizer.encode(text)\n",
    "        for i in range(0, len(tokens) - block_size + 1, block_size):\n",
    "            self.examples.append(tokens[i:i + block_size])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.examples[idx], dtype=torch.long)\n",
    "    \n",
    "dataset = TextDataset(text, tokenizer, block_size=128)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    step = 0\n",
    "    for batch in dataloader:\n",
    "        inputs = batch\n",
    "        labels = batch.clone()\n",
    "        train_model(model, inputs.to(device), labels.to(device), epochs=1, learning_rate=5e-5)  \n",
    "        step += 1\n",
    "        if step % 50 == 0:\n",
    "            saveCheckpoint(epoch,step,model,torch.optim.AdamW(model.parameters(), lr=5e-5),loss.item())\n",
    "            print(f\"Checkpoint saved at epoch {epoch}, step {step}\")\n",
    "            inference(model,\"Gravity is\", max_length=50)\n",
    "            inference(model,\"The meaning of life is\", max_length=50)\n",
    "            inference(model,\"Once upon a time\", max_length=50)\n",
    "saveCheckpoint(2,0,model,torch.optim.AdamW(model.parameters(), lr=5e-5),loss.item())\n",
    "loadCheckpoint(model,torch.optim.AdamW(model.parameters(), lr=5e-5),'checkpoint.pth20')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (decoder): LlamaDecoder(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (attention): CausalLMAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"New model built from scratch \",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43minference\u001b[49m(model,\u001b[33m\"\u001b[39m\u001b[33mGravity is\u001b[39m\u001b[33m\"\u001b[39m, max_length=\u001b[32m50\u001b[39m)\n\u001b[32m      2\u001b[39m inference(model,\u001b[33m\"\u001b[39m\u001b[33mThe meaning of life is\u001b[39m\u001b[33m\"\u001b[39m, max_length=\u001b[32m50\u001b[39m)\n\u001b[32m      3\u001b[39m inference(model,\u001b[33m\"\u001b[39m\u001b[33mOnce upon a time\u001b[39m\u001b[33m\"\u001b[39m, max_length=\u001b[32m50\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'inference' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "inference(model,\"Gravity is\", max_length=50)\n",
    "inference(model,\"The meaning of life is\", max_length=50)\n",
    "inference(model,\"Once upon a time\", max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
